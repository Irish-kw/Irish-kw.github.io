
<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Shao-Hua Sun</title>
  <meta name="description" lang="en" content="">
  <meta name="description" lang="cn" content="">
  <meta name="keywords" lang="en" content="" />
  <meta name="keywords" lang="cn" content="" />
  
  

  <link rel="shortcut icon" href="/images/icon/sh_logo.png">
  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="https://shaohua0116.github.io/">
  <link rel="alternate" type="application/rss+xml" title="Shao-Hua Sun" href="https://shaohua0116.github.io/feed.xml" />
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
  <script src="/js/jquery-2.1.3.min.js"> </script>

  <!--[if lt IE 9]>
<script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/r29/html5.min.js">
</script>
<![endif]-->



</head>


  <body>
    <!-- Google Tag Manager -->
<noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-K5ZTMW"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-K5ZTMW');</script>
<!-- End Google Tag Manager -->
    <header class="site-header">
  <div class="wrapper">
  </div>
</header>

    <div id="profile-cover" class="cover shallow-bg img-responsive">
  
  <div id="profile-namecard" class="profile-wrapper wrapper-light">
    <div id="my-pic" class="profile-col profile-col-1">
      <img id="profile-avatar" src="/images/profile.jpg" alt="Me" class="circle-img border-dark"/>
    </div>
    <div id="my-contact" class="profile-col profile-col-2">
      <div id="my-name" class="text-grey-dark">
        Shao-Hua Sun (孫紹華)
      </div>
      <div id="my-title" class="text-grey">
        Assistant Professor <br>at National Taiwan University
      </div>
      <div id="my-email" class="text-grey-light">
        <a href='mailto:shaohuas@ntu.edu.tw' >shaohuas@ntu.edu.tw</a>
      </div>
      <div class="social-media">
          <link rel="stylesheet" href="/images/icon/academicons/css/academicons.min.css"/>


<a href="https://github.com/shaohua0116" class="icon-button github" target="_blank">
  <i class="fa fa-github icon-github"></i>
  <span></span>
</a>



<a href="https://twitter.com/shaohua0116" class="icon-button twitter" target="_blank">
  <i class="fa fa-twitter icon-twitter"></i>
  <span></span>
</a>



<a href="https://linkedin.com/in/shaohua0116" class="icon-button linkedin" target="_blank">
  <i class="fa fa-linkedin icon-linkedin"></i>
  <span></span>
</a>



<a href="https://scholar.google.com/citations?hl=en&user=uXsfnaQAAAAJ" class="icon-button gscholar" target="_blank">
  <i class="ai ai-google-scholar icon-gscholar"></i>
  <span></span>
</a>


      </div>
    </div>
    <div id="my-desc" class="profile-col profile-col-2 hide">
      <div id="my-desc-title">
        Shao-Hua Sun
      </div>
      <div id="my-desc-content">
        My research interests span over the fields of <strong>Robot Learning</strong>, <strong>Reinforcement Learning</strong>, <strong>Machine Learning</strong>, and <strong>Program Synthesis</strong>.<br><br><br>
      </div>
    </div>
  </div>
</div>


    <div class="page-content">
      <div class="wrapper">
        <div id="bio" class="bio">
  <h1 class="md-heading text-center">
    Bio
  </h1>
  
  <div class="bio-body">
      I am an <strong>Assistant Professor</strong> at <strong>National Taiwan University (NTU)</strong> with a joint appointment in <strong>the Department of Electrical Engineering</strong> and <strong>the Graduate Institute of Communication Engineering</strong>. Prior to joining NTU, I recently completed my Ph.D. in Computer Science at the University of Southern California, where I worked in the <a href='http://clvrai.com/' target='_blank'>Cognitive Learning for Vision and Robotics Lab (CLVR)</a>. Before that, I received my B.S. degree in Electrical Engineering from NTU. My research interests span <strong>Robot Learning</strong>, <strong>Reinforcement Learning</strong>, <strong>Program Synthesis</strong>, and <strong>Machine Learning</strong>.
  </div>
  
  <div class="bio-body">
      <p><span style='color: #902a1f;'><strong>Hiring news</strong></span>: I am looking for students interested in machine learning, robot learning, reinforcement learning, and program synthesis. Specifically, I am hiring <strong>M.S.</strong> and <strong>Ph.D. students</strong> admitted to the Data Science and Smart Networking Group at the Graduate Institute of Communication Engineering (電信所丙組/資料科學與智慧網路組) or the Data Science Degree Program (資料科學學位學程) at NTU. Also, I am seeking <strong>undergraduate students</strong>, <strong>research assistants</strong>, and <strong>visiting students/scholars</strong> with different experience levels. If you are interested in joining my group, check out <a href='https://drive.google.com/file/d/13sMz1IDjLUrrZpF4c7_vz7HEaeuBWUww/view?usp=drive_link' target='_blank'>this slide</a> and contact me at <a href = 'mailto: shaohuas@ntu.edu.tw'>shaohuas@ntu.edu.tw</a> with your CV and transcript; <strong>undergraduate students</strong> please fill in <a href='https://docs.google.com/forms/d/e/1FAIpQLSdFg1ogZ1ounBhH4TbTtjlaIH7w3XMTnro02fBV7XUltUNbZA/viewform?usp=sf_link' target='_blank'>this form</a> instead.</p>
  </div>
  
  <div class="bio-bottom-margin">
  </div>
</div>

<!-- INDUCTRY -->
<div class="pub">
</div>
<div id="industry" class="industry">
  <h1 class="md-heading text-center">
    Industrial Outreach and Collaboration <!-- (產業界合作) -->
  </h1>
  <div class="industry-body">
    
    <div class="industry-item">
      <div class="industry-left">
        產學合作
      </div>
      <div class="industry-right">
        <b>TSMC</b> (台積電), <b>Nvidia</b> (輝達), <b>ASUS</b> (華碩)
      </div>
    </div>
    
    <div class="industry-item">
      <div class="industry-left">
        顧問
      </div>
      <div class="industry-right">
        <b>VICI Holdings</b> (威旭資訊) 機器學習技術顧問
      </div>
    </div>
    
  </div>
</div>
<div id="reachout" class="reachout">
  <div class="reachout-body">
      Please reach out to me at <a href = 'mailto: shaohuas@ntu.edu.tw'>shaohuas@ntu.edu.tw</a> if you are interested in setting up collaborations with us.
  </div>
</div>


<!-- NEWS -->
<div class="pub">
</div>
<div id="news" class="news">
  <h1 class="md-heading text-center">
    News
  </h1>
  <div class="news-body">
    
    <div class="news-item">
      <div class="news-left">
        <div class="news-date">
          Nov 2023
        </div>
      </div>
      <div class="news-right">
        <div class="news-news">
          I will give a tutorial on <a href="https://shaohua0116.github.io/acml2023_tutorial/"><b>Neural Program Synthesis and Induction</b></a> at <a href="https://www.acml-conf.org/2023/index.html"><b>ACML 2023</a></b>.
        </div>
      </div>
    </div>
    
    <div class="news-item">
      <div class="news-left">
        <div class="news-date">
          Aug 2023
        </div>
      </div>
      <div class="news-right">
        <div class="news-news">
          Our paper <b>Bootstrap Your Own Skills: Learning to Solve New Tasks with Large Language Model Guidance</b> is accepted by <b><a href="https://www.corl2023.org/">CoRL 2023</a></b> for an <b>oral</b> presentation.
        </div>
      </div>
    </div>
    
    <div class="news-item">
      <div class="news-left">
        <div class="news-date">
          June 2023
        </div>
      </div>
      <div class="news-right">
        <div class="news-news">
          I gave a talk at <a href='https://nthu-en.site.nthu.edu.tw/'>National Tsing Hua University</a> on <b><a href='https://drive.google.com/file/d/1Jwh59RQwYUQo9NNSE12oxA-mtbKq-9QD/view?usp=share_link'>Program-Guided Robot Learning</a></b> hosted by Prof. <a href='https://elsalab.ai/about'>Chun-Yi Lee</a> and Prof. <a href='https://theochiu.wixsite.com/nthucs'>Te-Chuan Chiu</a>.
        </div>
      </div>
    </div>
    
    <div class="news-item">
      <div class="news-left">
        <div class="news-date">
          May 2023
        </div>
      </div>
      <div class="news-right">
        <div class="news-news">
          I gave a talk at <a href='https://aiforum2023.cs.nthu.edu.tw/'>TAAI AI Forum 2023</a> on <b><a href='https://drive.google.com/file/d/1nzDcQkG1VUb4Xt36eTdwxNFRgs9cyJhK/view?usp=share_link'>Learning to Synthesize Programs as Interpretable and Generalizable Reinforcement Learning Policies</a></b> hosted by Prof. <a href='https://www.csie.ntu.edu.tw/~htlin/'>Hsuan-Tien Lin</a>, covering our works that re-formulate solving reinforcement Learning tasks as program synthesis problems.
        </div>
      </div>
    </div>
    
    <div class="news-item">
      <div class="news-left">
        <div class="news-date">
          May 2023
        </div>
      </div>
      <div class="news-right">
        <div class="news-news">
          Our paper <b>Hierarchical Programmatic Reinforcement Learning via Learning to Compose Programs</b> is accepted by <b><a href="https://icml.cc/Conferences/2023/Dates">ICML 2023</a></b>.
        </div>
      </div>
    </div>
    
  </div>
</div>

<!--
<div class="dissertation">
<div id="bio" class="bio">
  <h1 class="md-heading text-center">
    Ph.D. Dissertation
  </h1>
  <div class="bio-body">
      <strong>Title</strong>: Program-Guided Framework for Interpreting and Acquiring Complex Skills with Learning Robots <br>
      <div class="dissertation-abs">
        <p>
        <strong>Abstract</strong>: My research focuses on developing a robot learning framework that enables robots to acquire long-horizon and complex skills with hierarchical structures, such as furniture assembly and cooking. Specifically, I aim to devise a robot learning framework which is: (1) interpretable: by decoupling interpreting skill specifications (e.g. demonstrations, reward functions) and executing skills, (2) programmatic: by generalizing from simple instances to complex instances without additional learning, (3) hierarchical: by operating on a proper level of abstraction that enables human users to interpret high-level plans of robots allows for composing primitive skills to solve long-horizon tasks, and (4) modular: by being equipped with modules specialized in different functions (e.g. perception, action) which collaborate, allowing for better generalization. This dissertation discusses a series of projects toward building such an interpretable, programmatic, hierarchical, and modular robot learning framework.
        </p>
      </div>
      <strong>Dissertation committee</strong>: <a href='http://people.csail.mit.edu/lim/' target='_blank'>Professor Joseph J. Lim</a> (Chair), <a href='https://robotics.usc.edu/~gaurav/' target='_blank'>Professor Gaurav Sukhatme</a>, <a href='https://stefanosnikolaidis.net/' target='_blank'>Professor Stefanos Nikolaidis</a>, and <a href='https://viterbi.usc.edu/directory/faculty/Nguyen/Quan' target='_blank'>Professor Quan Nguyen</a> <br>
        <img class="dissertation-img" src="./dissertation/dissertation_fig.png"/>
        <div class="tags">
          
          [<a class="tag" href="/dissertation/defense_slide_Sun.pdf"> <strong>Defense Slide</strong> </a>]
          
          [<a class="tag" href="/dissertation/dissertation_Sun.pdf"> <strong>Dissertation</strong> </a>]
          
          [<a class="tag" href="/bibtex/dissertation.txt"> <strong>Bibtex</strong> </a>]
          
        </div>
  </div>
</div>
</div>
-->

<div class="research">
<div id="bio" class="bio">
  <h1 class="md-heading text-center">
    Research Highlight - Program-Guided Robot Learning
  </h1>
  <div class="bio-body">
      <div class="research-abs">
        <p>My research focuses on developing a robot learning framework that enables robots to acquire long-horizon and complex skills with hierarchical structures, such as furniture assembly and cooking. Specifically, I present an interpretable and generalizable <strong>program-guided robot learning</strong> framework, which represents desired behaviors as a program as well as acquires and utilizes primitive skills for learning to execute desired skills. Instead of learning in an end-to-end manner, I propose to design specialized learning modules that aim to (1) perform <strong>program inference</strong> to explicitly infer underlying programs that describe the skills of interest, (2) <strong>acquire primitive skills</strong> that can be used to compose more complex and longer-horizon skills, and (3) perform <strong>task execution</strong> by following the inferred program and utilizing acquired primitive skills to replicate the desired skills. <a href='https://drive.google.com/file/d/1Jwh59RQwYUQo9NNSE12oxA-mtbKq-9QD/view?usp=share_link' target='_blank'>This slide</a> gives an overview of my research.</p>
      </div>
        <img class="research-img" src="./images/research_highlight.jpeg"/>
  </div>
</div>
</div>

<div id="publications" class="publications">
  <h1 class="md-heading text-center">
    <!-- <i class="fa fa-file" aria-hidden="true"></i> -->
    Publications
  </h1>
  <div class="pub-list">
  
  
    <div class="pub">
      <div class="pub-left">
        <img class="intro-img frame" src="/images/projects/boss.gif"/>
      </div>
      <div class="pub-right">
        <div class="title">
          Bootstrap Your Own Skills: Learning to Solve New Tasks with Large Language Model Guidance
        </div>
        <div class="authors"><a class="author" href="https://jesbu1.github.io/">Jesse Zhang</a>,
          <a class="author" href="https://jiahui-3205.github.io/">Jiahui Zhang</a>,
          <a class="author" href="https://kpertsch.github.io/">Karl Pertsch</a>,
          <a class="author" href="https://www.linkedin.com/in/ziyi-liu-318616186/">Ziyi Liu</a>,
          <a class="author" href="https://shanzhenren.github.io/">Xiang Ren</a>,
          <a class="author" href="https://minsukchang.com/">Minsuk Chang</a>,
          <a class="author" href=""><strong>Shao-Hua Sun</strong></a>,
          <a class="author" href="https://clvrai.com/web_lim/">Joseph J. Lim</a>
          
        </div>

        
        <div class="publisher">
            <span class="publisher"> Conference on Robot Learning (CoRL) 2023</span>
            <span class="publisher_oral">&nbsp (Oral)</span>
        </div>
        
        
        <div class="desc">
          <p>
          
            Our approach BOSS (<strong>BO</strong>ot<strong>S</strong>trapping your own <strong>S</strong>kills) learns to accomplish new tasks by performing "skill bootstrapping," where an agent with a set of primitive skills interacts with the environment to practice new skills without receiving reward feedback for tasks outside of the initial skill set. This bootstrapping phase is guided by large language models that inform the agent of meaningful skills to chain together. Through this process, BOSS builds a wide range of complex and useful behaviors from a basic set of primitive skills. 
          
          </p>
        </div>
        
        <div class="publish">
          <!--
          <span class="publisher">Conference on Robot Learning (CoRL) 2023</span>
          -->
          <span class="status"></span>
          <span class="place"></span>
        </div>
        <div class="tags">
          
          [<a class="tag" href="https://openreview.net/pdf?id=a0mFRgadGO"> <strong>Paper</strong> </a>]
          
          [<a class="tag" href="https://clvrai.github.io/boss/"> <strong>Project Page</strong> </a>]
          
          [<a class="tag" href="/bibtex/boss.txt"> <strong>Bibtex</strong> </a>]
          
        </div>
      </div>
    </div>
    
    
  
    <div class="pub">
      <div class="pub-left">
        <img class="intro-img frame" src="/images/projects/hprl.png"/>
      </div>
      <div class="pub-right">
        <div class="title">
          Hierarchical Programmatic Reinforcement Learning via Learning to Compose Programs
        </div>
        <div class="authors"><a class="author" href="https://dannyliu15.github.io/">Guan-Ting Liu</a>*,
          <a class="author" href="https://guapaqaq.github.io/">En-Pei Hu</a>*,
          <a class="author" href="https://www.csie.ntu.edu.tw/~pjcheng/">Pu-Jen Cheng</a>,
          <a class="author" href="https://speech.ee.ntu.edu.tw/~hylee/index.php">Hung-Yi Lee</a>,
          <a class="author" href=""><strong>Shao-Hua Sun</strong></a>
          
        </div>

        
        <div class="publisher">
            <span class="publisher">International Conference on Machine Learning (ICML) 2023</span>
        </div>
        
        
        <div class="desc">
          <p>
          
            We re-formulate solving a reinforcement learning task as synthesizing a task-solving program that can be executed to interact with the environment and maximize the return. We first learn a program embedding space that continuously parameterizes a diverse set of programs sampled from a program dataset. Then, we train a meta-policy, whose action space is the learned program embedding space, to produce a series of programs (i.e., predict a series of actions) to yield a composed task-solving program.
          
          </p>
        </div>
        
        <div class="publish">
          <!--
          <span class="publisher">International Conference on Machine Learning (ICML) 2023</span>
          -->
          <span class="status"></span>
          <span class="place"></span>
        </div>
        <div class="tags">
          
          [<a class="tag" href="https://arxiv.org/abs/2301.12950"> <strong>Paper</strong> </a>]
          
          [<a class="tag" href="https://nturobotlearninglab.github.io/hprl/"> <strong>Project Page</strong> </a>]
          
          [<a class="tag" href="https://nturobotlearninglab.github.io/hprl/asset/hprl_slide.pdf"> <strong>Slide</strong> </a>]
          
          [<a class="tag" href="https://nturobotlearninglab.github.io/hprl/asset/hprl_poster.png"> <strong>Poster</strong> </a>]
          
          [<a class="tag" href="/bibtex/hprl.txt"> <strong>Bibtex</strong> </a>]
          
        </div>
      </div>
    </div>
    
    
  
    <div class="pub">
      <div class="pub-left">
        <img class="intro-img frame" src="/images/projects/locavqg.png"/>
      </div>
      <div class="pub-right">
        <div class="title">
          Location-aware Visual Question Generation with Lightweight Models
        </div>
        <div class="authors"><a class="author">Nicholas Collin Suwono</a>,
          <a class="author" href="https://dinobby.github.io/">Justin Chih-Yao Chen</a>,
          <a class="author">Tun Min Hung</a>,
          <a class="author" href="https://crowd.ist.psu.edu/">Ting-Hao Kenneth Huang</a>,
          <a class="author">I-Bin Liao</a>,
          <a class="author">Yung-Hui Li</a>,
          <a class="author" href="https://academiasinicanlplab.github.io/">Lun-Wei Ku</a>,
          <a class="author" href=""><strong>Shao-Hua Sun</strong></a>
          
        </div>

        
        <div class="publisher">
            <span class="publisher">Empirical Methods in Natural Language Processing (EMNLP) 2023</span>
        </div>
        
        
        <div class="desc">
          <p>
          
            This work introduces a novel task, location-aware visual question generation (LocaVQG), which aims to generate engaging questions from data relevant to a particular geographical location (e.g.,surrounding images and its GPS coordinate). To tackle this task, we present a dataset generation pipeline that leverages GPT-4 to produce diverse and sophisticated questions. We propose methods to train lightweight models which can reliably generate engaging questions from location-aware information. 
          
          </p>
        </div>
        
        <div class="publish">
          <!--
          <span class="publisher">Empirical Methods in Natural Language Processing (EMNLP) 2023</span>
          -->
          <span class="status"></span>
          <span class="place"></span>
        </div>
        <div class="tags">
          
          [<a class="tag" href="https://arxiv.org/abs/2310.15129"> <strong>Paper</strong> </a>]
          
          [<a class="tag" href="https://github.com/AcademiaSinicaNLPLab/LocaVQG"> <strong>Project Page</strong> </a>]
          
          [<a class="tag" href="/bibtex/locavqg.txt"> <strong>Bibtex</strong> </a>]
          
        </div>
      </div>
    </div>
    
    
  
    <div class="pub">
      <div class="pub-left">
        <img class="intro-img frame" src="/images/projects/dbc.png"/>
      </div>
      <div class="pub-right">
        <div class="title">
          Diffusion Model-Augmented Behavioral Cloning
        </div>
        <div class="authors"><a class="author">Hsiang-Chun Wang</a>,
          <a class="author">Shang-Fu Chen</a>,
          <a class="author" href="https://qaz159qaz159.github.io/">Ming-Hao Hsu</a>,
          <a class="author">Chun-Mao Lai</a>,
          <a class="author" href=""><strong>Shao-Hua Sun</strong></a>
          
        </div>

        
        <div class="publisher">
            <span class="publisher">Frontiers4LCD Workshop at International Conference on Machine Learning (ICML) 2023</span>
        </div>
        
        
        <div class="desc">
          <p>
          
            This work aims to augment BC by employing diffusion models for modeling expert behaviors, and designing a learning objective that leverages learned diffusion models to guide policy learning. To this end, we propose diffusion model-augmented behavioral cloning (Diffusion-BC) that combines our proposed diffusion model guided learning objective with the BC objective, which complements each other. Our proposed method outperforms baselines or achieves competitive performance in various continuous control domains, including navigation, robot arm manipulation, and locomotion.
          
          </p>
        </div>
        
        <div class="publish">
          <!--
          <span class="publisher">Frontiers4LCD Workshop at International Conference on Machine Learning (ICML) 2023</span>
          -->
          <span class="status"></span>
          <span class="place"></span>
        </div>
        <div class="tags">
          
          [<a class="tag" href="https://arxiv.org/abs/2302.13335"> <strong>Paper</strong> </a>]
          
          [<a class="tag" href="https://nturobotlearninglab.github.io/dbc/"> <strong>Project Page</strong> </a>]
          
          [<a class="tag" href="https://nturobotlearninglab.github.io/dbc/asset/dbc_poster_icml2023_f4lcd.pdf"> <strong>Poster</strong> </a>]
          
          [<a class="tag" href="/bibtex/dbc.txt"> <strong>Bibtex</strong> </a>]
          
        </div>
      </div>
    </div>
    
    
  
    
  
    <div class="pub">
      <div class="pub-left">
        <img class="intro-img frame" src="/images/projects/qmp.png"/>
      </div>
      <div class="pub-right">
        <div class="title">
          Efficient Multi-Task Reinforcement Learning via Selective Behavior Sharing
        </div>
        <div class="authors"><a class="author" href="https://gracehzhang.github.io/">Grace Zhang</a>,
          <a class="author" href="https://ayushj240.github.io/">Ayush Jain</a>,
          <a class="author" href="https://hij0527.github.io/">Injune Hwang</a>,
          <a class="author" href=""><strong>Shao-Hua Sun</strong></a>,
          <a class="author" href="http://www-bcf.usc.edu/~limjj/">Joseph J. Lim</a>
          
        </div>

        
        <div class="publisher">
            <span class="publisher">Deep RL Workshop at Neural Information Processing Systems (NeurIPS) 2022</span>
        </div>
        
        
        <div class="desc">
          <p>
          
            We propose a multi-task reinforcement learning method, Q-switch Mixture of policies (QMP), that can share exploratory behavior, which can be helpful even when the optimal behaviors differ. Furthermore, as we learn each task, we can guide the exploration by sharing behaviors in a task and state dependent way. QMP learns to selectively share exploratory behavior between tasks by using a mixture of policies based on estimated discounted returns to gather training data. 
          
          </p>
        </div>
        
        <div class="publish">
          <!--
          <span class="publisher">Deep RL Workshop at Neural Information Processing Systems (NeurIPS) 2022</span>
          -->
          <span class="status"></span>
          <span class="place"></span>
        </div>
        <div class="tags">
          
          [<a class="tag" href="https://arxiv.org/abs/2302.00671"> <strong>Paper</strong> </a>]
          
          [<a class="tag" href="https://sites.google.com/view/qmp-mtrl"> <strong>Project Page</strong> </a>]
          
          [<a class="tag" href="https://openreview.net/forum?id=U3n8WPtKPm"> <strong>OpenReview</strong> </a>]
          
          [<a class="tag" href="/bibtex/qmp.txt"> <strong>Bibtex</strong> </a>]
          
        </div>
      </div>
    </div>
    
    
  
    <div class="pub">
      <div class="pub-left">
        <img class="intro-img frame" src="/images/projects/simpl.png"/>
      </div>
      <div class="pub-right">
        <div class="title">
          Skill-based Meta-Reinforcement Learning
        </div>
        <div class="authors"><a class="author" href="https://www.mlai-kaist.com/people">Taewook Nam</a>,
          <a class="author" href=""><strong>Shao-Hua Sun</strong></a>,
          <a class="author" href="https://kpertsch.github.io/">Karl Pertsch</a>,
          <a class="author" href="http://www.sungjuhwang.com/">Sung Ju Hwang</a>,
          <a class="author" href="http://www-bcf.usc.edu/~limjj/">Joseph J. Lim</a>
          
        </div>

        
        <div class="publisher">
            <span class="publisher">International Conference on Learning Representations (ICLR) 2022</span>
        </div>
        
        
        <div class="desc">
          <p>
          
            We devise a method that enables meta-learning on long-horizon, sparse-reward tasks, allowing us to solve unseen target tasks with orders of magnitude fewer environment interactions. Specifically, we propose to (1) extract reusable skills and a skill prior from offline datasets, (2) meta-train a high-level policy that learns to efficiently compose learned skills into long-horizon behaviors, and (3) rapidly adapt the meta-trained policy to solve an unseen target task.
          
          </p>
        </div>
        
        <div class="publish">
          <!--
          <span class="publisher">International Conference on Learning Representations (ICLR) 2022</span>
          -->
          <span class="status"></span>
          <span class="place"></span>
        </div>
        <div class="tags">
          
          [<a class="tag" href="https://openreview.net/pdf?id=jeLW-Fh9bV"> <strong>Paper</strong> </a>]
          
          [<a class="tag" href="https://namsan96.github.io/SiMPL/"> <strong>Project Page</strong> </a>]
          
          [<a class="tag" href="https://github.com/namsan96/simpl"> <strong>Code</strong> </a>]
          
          [<a class="tag" href="https://openreview.net/forum?id=jeLW-Fh9bV"> <strong>OpenReview</strong> </a>]
          
          [<a class="tag" href="/bibtex/simpl.txt"> <strong>Bibtex</strong> </a>]
          
        </div>
      </div>
    </div>
    
    
  
    <div class="pub">
      <div class="pub-left">
        <img class="intro-img frame" src="/images/projects/leaps.png"/>
      </div>
      <div class="pub-right">
        <div class="title">
          Learning to Synthesize Programs as Interpretable and Generalizable Policies
        </div>
        <div class="authors"><a class="author">Dweep Trivedi</a>*,
          <a class="author" href="https://jesbu1.github.io/">Jesse Zhang</a>*,
          <a class="author" href=""><strong>Shao-Hua Sun</strong></a>,
          <a class="author" href="http://www-bcf.usc.edu/~limjj/">Joseph J. Lim</a>
          
        </div>

        
        <div class="publisher">
            <span class="publisher">Neural Information Processing Systems (NeurIPS) 2021</span>
        </div>
        
        
        <div class="desc">
          <p>
          
            We present a framework that learns to synthesize a program, detailing the procedure to solve a task in a flexible and expressive manner, solely from reward signals. To alleviate the difficulty of learning to compose programs to induce the desired agent behavior from scratch, we propose to learn a program embedding space that continuously parameterizes diverse behaviors in an unsupervised manner and then search over the learned program embedding space to yield a program that maximizes the return for a given task.
          
          </p>
        </div>
        
        <div class="publish">
          <!--
          <span class="publisher">Neural Information Processing Systems (NeurIPS) 2021</span>
          -->
          <span class="status"></span>
          <span class="place"></span>
        </div>
        <div class="tags">
          
          [<a class="tag" href="https://arxiv.org/abs/2108.13643"> <strong>Paper</strong> </a>]
          
          [<a class="tag" href="https://clvrai.github.io/leaps/"> <strong>Project Page</strong> </a>]
          
          [<a class="tag" href="https://github.com/clvrai/leaps"> <strong>Code</strong> </a>]
          
          [<a class="tag" href="./images/slides/leaps.pdf"> <strong>Slide</strong> </a>]
          
          [<a class="tag" href="/bibtex/leaps.txt"> <strong>Bibtex</strong> </a>]
          
        </div>
      </div>
    </div>
    
    
  
    <div class="pub">
      <div class="pub-left">
        <img class="intro-img frame" src="/images/projects/goal.png"/>
      </div>
      <div class="pub-right">
        <div class="title">
          Generalizable Imitation Learning from Observation via Inferring Goal Proximity
        </div>
        <div class="authors"><a class="author" href="http://youngwoon.github.io/">Youngwoon Lee</a>,
          <a class="author" href="https://www.andrewszot.com/">Andrew Szot</a>,
          <a class="author" href=""><strong>Shao-Hua Sun</strong></a>,
          <a class="author" href="http://www-bcf.usc.edu/~limjj/">Joseph J. Lim</a>
          
        </div>

        
        <div class="publisher">
            <span class="publisher">Neural Information Processing Systems (NeurIPS) 2021</span>
        </div>
        
        
        <div class="desc">
          <p>
          
            Task progress is intuitive and readily available task information that can guide an agent closer to the desired goal. Furthermore, a progress estimator can generalize to new situations. From this intuition, we propose a simple yet effective imitation learning from observation method for a goal-directed task using a learned goal proximity function as a task progress estimator, for better generalization to unseen states and goals. We obtain this goal proximity function from expert demonstrations and online agent experience, and then use the learned goal proximity as a dense reward for policy training.
          
          </p>
        </div>
        
        <div class="publish">
          <!--
          <span class="publisher">Neural Information Processing Systems (NeurIPS) 2021</span>
          -->
          <span class="status"></span>
          <span class="place"></span>
        </div>
        <div class="tags">
          
          [<a class="tag" href="https://papers.nips.cc/paper/2021/hash/868b7df964b1af24c8c0a9e43a330c6a-Abstract.html"> <strong>Paper</strong> </a>]
          
          [<a class="tag" href="https://clvrai.github.io/goal_prox_il/"> <strong>Project Page</strong> </a>]
          
          [<a class="tag" href="https://github.com/clvrai/goal_prox_il"> <strong>Code</strong> </a>]
          
          [<a class="tag" href="/images/slides/goal_proximity.pdf"> <strong>Slide</strong> </a>]
          
          [<a class="tag" href="/bibtex/goal.txt"> <strong>Bibtex</strong> </a>]
          
        </div>
      </div>
    </div>
    
    
  
    <div class="pub">
      <div class="pub-left">
        <img class="intro-img frame" src="/images/projects/pga.png"/>
      </div>
      <div class="pub-right">
        <div class="title">
          Program Guided Agent
        </div>
        <div class="authors"><a class="author" href=""><strong>Shao-Hua Sun</strong></a>,
          <a class="author">Te-Lin Wu</a>,
          <a class="author" href="http://www-bcf.usc.edu/~limjj/">Joseph J. Lim</a>
          
        </div>

        
        <div class="publisher">
            <span class="publisher"> International Conference on Learning Representations (ICLR) 2020</span>
            <span class="publisher_oral">&nbsp (Spotlight)</span>
        </div>
        
        
        <div class="desc">
          <p>
          
            We propose to utilize programs, structured in a formal language, as a precise and expressive way to specify tasks, instead of natural languages which can often be ambiguous. We then devise a modular framework that learns to perform a task specified by a program – as different circumstances give rise to diverse ways to accomplish the task, our framework can perceive which circumstance it is currently under, and instruct a multitask policy accordingly to fulfill each subtask of the overall task.
          
          </p>
        </div>
        
        <div class="publish">
          <!--
          <span class="publisher">International Conference on Learning Representations (ICLR) 2020</span>
          -->
          <span class="status"></span>
          <span class="place"></span>
        </div>
        <div class="tags">
          
          [<a class="tag" href="https://openreview.net/pdf?id=BkxUvnEYDH"> <strong>Paper</strong> </a>]
          
          [<a class="tag" href="https://shaohua0116.github.io/ProgramGuidedAgent/"> <strong>Project Page</strong> </a>]
          
          [<a class="tag" href="https://openreview.net/forum?id=BkxUvnEYDH"> <strong>OpenReview</strong> </a>]
          
          [<a class="tag" href="./images/slides/pga.pdf"> <strong>Slide</strong> </a>]
          
          [<a class="tag" href="/bibtex/pga.txt"> <strong>Bibtex</strong> </a>]
          
        </div>
      </div>
    </div>
    
    
  
    <div class="pub">
      <div class="pub-left">
        <img class="intro-img frame" src="/images/projects/mmaml.png"/>
      </div>
      <div class="pub-right">
        <div class="title">
          Multimodal Model-Agnostic Meta-Learning via Task-Aware Modulation
        </div>
        <div class="authors"><a class="author" href="https://vuoristo.github.io/">Risto Vuorio</a>*,
          <a class="author" href=""><strong>Shao-Hua Sun</strong></a>*,
          <a class="author" href="http://hexianghu.com/">Hexiang Hu</a>,
          <a class="author" href="http://www-bcf.usc.edu/~limjj/">Joseph J. Lim</a>
          
        </div>

        
        <div class="publisher">
            <span class="publisher"> Neural Information Processing Systems (NeurIPS) 2019</span>
            <span class="publisher_oral">&nbsp (Spotlight)</span>
        </div>
        
        
        <div class="desc">
          <p>
          
            Model-agnostic meta-learners aim to acquire meta-prior parameters from a distribution of tasks and adapt to novel tasks with few gradient updates. Yet, seeking a common initialization shared across the entire task distribution substantially limits the diversity of the task distributions that they are able to learn from. We propose a multimodal MAML (MMAML) framework, which is able to modulate its meta-learned prior according to the identified mode, allowing more efficient fast adaptation.
          
          </p>
        </div>
        
        <div class="publish">
          <!--
          <span class="publisher">Neural Information Processing Systems (NeurIPS) 2019</span>
          -->
          <span class="status"></span>
          <span class="place"></span>
        </div>
        <div class="tags">
          
          [<a class="tag" href="http://papers.nips.cc/paper/8296-multimodal-model-agnostic-meta-learning-via-task-aware-modulation"> <strong>Paper</strong> </a>]
          
          [<a class="tag" href="https://vuoristo.github.io/MMAML/"> <strong>Project Page</strong> </a>]
          
          [<a class="tag" href="https://github.com/shaohua0116/MMAML-Classification"> <strong>Code</strong> </a>]
          
          [<a class="tag" href="./images/posters/mmaml.pdf"> <strong>Poster</strong> </a>]
          
          [<a class="tag" href="./images/slides/mmaml.pdf"> <strong>Spotlight Slide</strong> </a>]
          
          [<a class="tag" href="https://slideslive.com/38921753/track-3-session-4"> <strong>Spotlight Talk</strong> </a>]
          
          [<a class="tag" href="/bibtex/mmaml.txt"> <strong>Bibtex</strong> </a>]
          
        </div>
      </div>
    </div>
    
    
  
    <div class="pub">
      <div class="pub-left">
        <img class="intro-img frame" src="/images/projects/fal.png"/>
      </div>
      <div class="pub-right">
        <div class="title">
          Feedback Adversarial Learning: Spatial Feedback for Improving Generative Adversarial Networks
        </div>
        <div class="authors"><a class="author" href="http://minyounghuh.com/">Minyoung Huh</a>*,
          <a class="author" href=""><strong>Shao-Hua Sun</strong></a>*,
          <a class="author" href="https://people.eecs.berkeley.edu/~nzhang/">Ning Zhang</a>
          
        </div>

        
        <div class="publisher">
            <span class="publisher">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2019</span>
        </div>
        
        
        <div class="desc">
          <p>
          
            We propose feedback adversarial learning (FAL) framework that can improve existing generative adversarial networks by leveraging spatial feedback from the discriminator. We formulate the generation task as a recurrent framework, in which the generator conditions on the discriminator spatial output response and its previous generation to improve generation quality over time - allowing the generator to attend and fix its previous mistakes. 
          
          </p>
        </div>
        
        <div class="publish">
          <!--
          <span class="publisher">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2019</span>
          -->
          <span class="status"></span>
          <span class="place"></span>
        </div>
        <div class="tags">
          
          [<a class="tag" href="http://openaccess.thecvf.com/content_CVPR_2019/html/Huh_Feedback_Adversarial_Learning_Spatial_Feedback_for_Improving_Generative_Adversarial_Networks_CVPR_2019_paper.html"> <strong>Paper</strong> </a>]
          
          [<a class="tag" href="./images/posters/fal.pdf"> <strong>Poster</strong> </a>]
          
          [<a class="tag" href="/bibtex/fal.txt"> <strong>Bibtex</strong> </a>]
          
        </div>
      </div>
    </div>
    
    
  
    <div class="pub">
      <div class="pub-left">
        <img class="intro-img frame" src="/images/projects/transition.png"/>
      </div>
      <div class="pub-right">
        <div class="title">
          Composing Complex Skills by Learning Transition Policies
        </div>
        <div class="authors"><a class="author" href="http://youngwoon.github.io/">Youngwoon Lee</a>*,
          <a class="author" href=""><strong>Shao-Hua Sun</strong></a>*,
          <a class="author" href="http://srirams32.github.io/">Sriram Somasundaram</a>,
          <a class="author" href="https://edwardshu.com/">Edward Hu</a>,
          <a class="author" href="http://www-bcf.usc.edu/~limjj/">Joseph J. Lim</a>
          
        </div>

        
        <div class="publisher">
            <span class="publisher">International Conference on Learning Representations (ICLR) 2019</span>
        </div>
        
        
        <div class="desc">
          <p>
          
            Humans acquire complex skills by exploiting previously learned skills and making transitions between them. To empower machines with this ability, we propose a method that can learn transition policies which effectively connect primitive skills to perform sequential tasks without handcrafted rewards. To efficiently train our transition policies, we introduce proximity predictors which induce rewards gauging proximity to suitable initial states for the next skill. 
          
          </p>
        </div>
        
        <div class="publish">
          <!--
          <span class="publisher">International Conference on Learning Representations (ICLR) 2019</span>
          -->
          <span class="status"></span>
          <span class="place"></span>
        </div>
        <div class="tags">
          
          [<a class="tag" href="https://openreview.net/pdf?id=rygrBhC5tQ"> <strong>Paper</strong> </a>]
          
          [<a class="tag" href="https://youngwoon.github.io/transition/"> <strong>Project Page</strong> </a>]
          
          [<a class="tag" href="https://github.com/youngwoon/transition"> <strong>Code</strong> </a>]
          
          [<a class="tag" href="https://openreview.net/forum?id=rygrBhC5tQ"> <strong>OpenReview</strong> </a>]
          
          [<a class="tag" href="./images/slides/transition.pdf"> <strong>Slide</strong> </a>]
          
          [<a class="tag" href="./images/posters/transition.pdf"> <strong>Poster</strong> </a>]
          
          [<a class="tag" href="/bibtex/transition.txt"> <strong>Bibtex</strong> </a>]
          
        </div>
      </div>
    </div>
    
    
  
    
  
    <div class="pub">
      <div class="pub-left">
        <img class="intro-img frame" src="/images/projects/view.png"/>
      </div>
      <div class="pub-right">
        <div class="title">
          Multi-view to Novel View: Synthesizing Novel Views with Self-Learned Confidence
        </div>
        <div class="authors"><a class="author" href=""><strong>Shao-Hua Sun</strong></a>,
          <a class="author" href="http://minyounghuh.com/">Minyoung Huh</a>,
          <a class="author" href="https://andrewliao11.github.io/">Yuan-Hong Liao</a>,
          <a class="author" href="https://people.eecs.berkeley.edu/~nzhang/">Ning Zhang</a>,
          <a class="author" href="http://www-bcf.usc.edu/~limjj/">Joseph J. Lim</a>
          
        </div>

        
        <div class="publisher">
            <span class="publisher">European Conference on Computer Vision (ECCV) 2018</span>
        </div>
        
        
        <div class="desc">
          <p>
          
            We aim to synthesize a target image with an arbitrary camera pose from multipple given source images. We propose an end-to-end trainable framework which consists of a flow prediction module and a pixel generation module to directly leverage information presented in source views as well as hallucinate missing pixels from statistical priors. We introduce a self-learned confidence aggregation mechanism to merge the predictions produced by the two modules given multi-view source images.
          
          </p>
        </div>
        
        <div class="publish">
          <!--
          <span class="publisher">European Conference on Computer Vision (ECCV) 2018</span>
          -->
          <span class="status"></span>
          <span class="place"></span>
        </div>
        <div class="tags">
          
          [<a class="tag" href="https://shaohua0116.github.io/Multiview2Novelview/sun2018multiview.pdf"> <strong>Paper</strong> </a>]
          
          [<a class="tag" href="https://shaohua0116.github.io/Multiview2Novelview"> <strong>Project Page</strong> </a>]
          
          [<a class="tag" href="https://github.com/shaohua0116/Multiview2Novelview"> <strong>Code</strong> </a>]
          
          [<a class="tag" href="./images/posters/view.pdf"> <strong>Poster</strong> </a>]
          
          [<a class="tag" href="/bibtex/multiview.txt"> <strong>Bibtex</strong> </a>]
          
        </div>
      </div>
    </div>
    
    
  
    <div class="pub">
      <div class="pub-left">
        <img class="intro-img frame" src="/images/projects/demo2program.png"/>
      </div>
      <div class="pub-right">
        <div class="title">
          Neural Program Synthesis from Diverse Demonstration Videos
        </div>
        <div class="authors"><a class="author" href=""><strong>Shao-Hua Sun</strong></a>*,
          <a class="author" href="http://cvlab.postech.ac.kr/~hyeonwoonoh/">Hyeonwoo Noh</a>*,
          <a class="author" href="http://srirams32.github.io/">Sriram Somasundaram</a>,
          <a class="author" href="http://www-bcf.usc.edu/~limjj/">Joseph J. Lim</a>
          
        </div>

        
        <div class="publisher">
            <span class="publisher">International Conference on Machine Learning (ICML) 2018</span>
        </div>
        
        
        <div class="desc">
          <p>
          
            Interpreting decision making logic in demonstration videos is key to collaborating with and mimicking humans. To empower machines with this ability, we propose a framework that is able to explicitly synthesize underlying programs from behaviorally diverse and visually complicated demonstration videos. We introduce a summarizer module to improve the network’s ability to integrate multiple demonstrations and employ a multi-task objective to encourage the model to learn meaningful intermediate representations.
          
          </p>
        </div>
        
        <div class="publish">
          <!--
          <span class="publisher">International Conference on Machine Learning (ICML) 2018</span>
          -->
          <span class="status"></span>
          <span class="place"></span>
        </div>
        <div class="tags">
          
          [<a class="tag" href="https://shaohua0116.github.io/demo2program/sun2018neural.pdf"> <strong>Paper</strong> </a>]
          
          [<a class="tag" href="https://shaohua0116.github.io/demo2program/"> <strong>Project Page</strong> </a>]
          
          [<a class="tag" href="https://github.com/shaohua0116/demo2program"> <strong>Code</strong> </a>]
          
          [<a class="tag" href="./images/slides/demo2program.pdf"> <strong>Slide</strong> </a>]
          
          [<a class="tag" href="./images/posters/demo2program.pdf"> <strong>Poster</strong> </a>]
          
          [<a class="tag" href="/bibtex/demo2program.txt"> <strong>Bibtex</strong> </a>]
          
        </div>
      </div>
    </div>
    
    
  
    
</div>

<!--
<div class="button">
  <h1 class="sm-heading text-center">
    <a href="/publications/#publications">
      <i class="fa fa-info-circle"></i> Full Publication List
    </a>
  </h1>
</div> 
-->

<!--
<div id="timeline" class="timeline-brief">
  <h1 class="md-heading text-center">
    Timeline
  </h1>

  <div class="timeline-body">
    
    
    
    
    
    
    <div class="timeline-item">
      <div class="timeline-date">
        Spring 2017 - present
      </div>
      <div class="timeline-title">
        <strong>PhD student</strong><br>University of Southern California
      </div>
      <div class="timeline-desc">
        Computer Vision, Deep Learning, Reinforcement Learning
      </div>
      
      <div class="timeline-host">
        Advisor: <a href='http://people.csail.mit.edu/lim/'>Prof. Joseph J. Lim</a>
      </div>
      
      
      <div class="timeline-host">
        Lab: <a href='http://clvrai.com/'>Cognitive Learning for Vision and Robotics Lab</a>
      </div>
      
    </div>
    
    
    
    <div class="timeline-item">
      <div class="timeline-date">
        Undergrad.
      </div>
      <div class="timeline-title">
        <strong>Research Assistant</strong><br>Research Center for Information Technology Innovation, Academia Sinica, Taiwan
      </div>
      <div class="timeline-desc">
        Computer Vision, Machine Learning, Computational Photography
      </div>
      
      <div class="timeline-host">
        Advisor: <a href='http://vllab.ee.ntu.edu.tw/members.html'>Prof. Yu-Chiang Frank Wang</a>
      </div>
      
      
      <div class="timeline-host">
        Lab: <a href='http://vllab.ee.ntu.edu.tw/'>Vison and Learning Lab</a>
      </div>
      
    </div>
    
    
    <div class="timeline-item">
    </div>
  </div>
</div>

<div class="button">
  <h1 class="sm-heading text-center">
    <a href="/timeline/#timeline">
      <i class="fa fa-info-circle"></i> Full Timeline
    </a>
  </h1>
</div> 
-->

<!--
<div id="implementation" class="implementation">
  <h1 class="md-heading text-center">
  Open Source Projects
  </h1>
  <div class="implementation-body">
    
      <div class="implementation-category">
          <span> Computer Vision </span>
      </div>
      
        <div class="implementation-item">
          <a class="implementation-role" href="https://github.com/shaohua0116/MultiDigitMNIST"> Multi-digit MNIST </a>
          <span class="implementation-activity"> Combine multiple MNIST digits to create 100/1000 classes for few-shot learning </span>
        </div>
      
        <div class="implementation-item">
          <a class="implementation-role" href="https://github.com/shaohua0116/NovelViewSynthesis-TensorFlow"> Novel View Synthesis </a>
          <span class="implementation-activity"> A simple novel view synthesis model trained on ShapeNet, KITTI, and Synthia datasets </span>
        </div>
      
        <div class="implementation-item">
          <a class="implementation-role" href="https://github.com/gitlimlab/Representation-Learning-by-Learning-to-Count"> Learning to Count </a>
          <span class="implementation-activity"> Learning representations by counting visual primitives presented in images </span>
        </div>
      
    
      <div class="implementation-category">
          <span> Generative Model </span>
      </div>
      
        <div class="implementation-item">
          <a class="implementation-role" href="https://github.com/gitlimlab/SSGAN-Tensorflow"> Semi-supervised GAN </a>
          <span class="implementation-activity"> Semi-supervised learning generative adversarial networks </span>
        </div>
      
        <div class="implementation-item">
          <a class="implementation-role" href="https://github.com/shaohua0116/WGAN-GP-TensorFlow"> WGAN-GP </a>
          <span class="implementation-activity"> Wasserstein GAN with gradient penalt and Least Squares GAN </span>
        </div>
      
        <div class="implementation-item">
          <a class="implementation-role" href="https://github.com/shaohua0116/DCGAN-Tensorflow"> DCGAN </a>
          <span class="implementation-activity"> Deep convolutional generative adversarial networks </span>
        </div>
      
        <div class="implementation-item">
          <a class="implementation-role" href="https://github.com/gitlimlab/Generative-Latent-Optimization-Tensorflow"> Generative Latent Optimization </a>
          <span class="implementation-activity"> Train a generator without the need to learn a discriminator </span>
        </div>
      
    
      <div class="implementation-category">
          <span> Tutorial </span>
      </div>
      
        <div class="implementation-item">
          <a class="implementation-role" href="https://github.com/shaohua0116/ICLR2019-OpenReviewData"> OpenReviewer Cralwer </a>
          <span class="implementation-activity"> Crawl data from OpenReview webpages and install Selenium and ChromeDriver </span>
        </div>
      
        <div class="implementation-item">
          <a class="implementation-role" href="https://github.com/shaohua0116/VAE-Tensorflow"> VAE Tutorial </a>
          <span class="implementation-activity"> Variational autoencoders for the deep learning course at USC </span>
        </div>
      
    
      <div class="implementation-category">
          <span> Misc </span>
      </div>
      
        <div class="implementation-item">
          <a class="implementation-role" href="https://github.com/shaohua0116/Group-Normalization-Tensorflow"> Group Normalization </a>
          <span class="implementation-activity"> Group normalization on the task of image classification </span>
        </div>
      
        <div class="implementation-item">
          <a class="implementation-role" href="https://github.com/gitlimlab/Relation-Network-Tensorflow"> Relation Networks </a>
          <span class="implementation-activity"> Relational networks and a script to generate Sort-of-CLEVR (a VQA dataset) </span>
        </div>
      
    
  </div>
</div>

<div id="service" class="service">
  <h1 class="md-heading text-center">
    Teaching
  </h1>
  <div class="service-body">
    
    <div class="service-item">
      <span class="service-role">Fall 2019</span>
      <span class="service-activity">CSCI-566 Deep Learning and its Application, Teaching Assistant, USC</span>
    </div>
    
    <div class="service-item">
      <span class="service-role">Spring 2019</span>
      <span class="service-activity">CSCI-599 Deep Learning and its Application, Teaching Assistant, USC</span>
    </div>
    
    <div class="service-item">
      <span class="service-role">Fall 2017</span>
      <span class="service-activity">CSCI-599 Deep Learning and its Application, Teaching Assistant, USC</span>
    </div>
    
    <div class="service-item">
      <span class="service-role">Fall 2016</span>
      <span class="service-activity">CSCI-570 Analysis of Algorithms, Teaching Assistant, USC</span>
    </div>
    
  </div>
</div>
-->

<div class="pub">
</div>
<div id="service" class="service">
  <h1 class="md-heading text-center">
    Professional Activity
  </h1>
  <div class="service-body">
    
    <div class="service-item">
      <span class="service-role">Conference reviewer</span>
      <span class="service-activity">NeurIPS, ICML, ICLR, CoRL, CVPR, ICCV, ECCV, AAAI, HRI, WACV, BMVC, ICIP</span>
    </div>
    
    <div class="service-item">
      <span class="service-role">Journal reviewer</span>
      <span class="service-activity">Transactions on Machine Learning Research, Machine Learning, IEEE Transactions on Image Processing, IEEE Transactions on Industrial Informatics, IEEE Transactions on Multimedia, ACM Transactions on Multimedia Computing Communications and Applications</span>
    </div>
    
  </div>
</div>

</div>

      </div>
    </div>

    
<footer class="site-footer">
  <div class="wrapper">
    <div class="footer-col">
      
<a href="https://github.com/shaohua0116" target="_blank">
  <img src="/images/icon/color-github.png" class="social-icon">
</a>



<a href="https://twitter.com/shaohua0116" target="_blank">
  <img src="/images/icon/color-twitter.png" class="social-icon">
</a>



<a href="https://linkedin.com/in/shaohua0116" target="_blank">
  <img src="/images/icon/color-linkedin.png" class="social-icon">
</a>



<a href="https://scholar.google.com/citations?hl=en&user=uXsfnaQAAAAJ" target="_blank">
  <img src="/images/icon/color-gscholar-footer.png" class="social-icon">
</a>


    </div>
    <div class="footer-col">
      <div class="contact-list">
          <p> &copy <script>document.write(new Date().getFullYear())</script>
           Shao-Hua Sun (孫紹華)</p>
          <!--
           Shao-Hua Sun (孫紹華)'s personal website</p>
          -->
      </div>
    </div>
  </div>
</footer>


  </body>
  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-101239130-1', 'auto');
  ga('send', 'pageview');

</script>

</html>
